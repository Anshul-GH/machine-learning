{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_02_end.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPvX2LwJ8eTJmcZXJdbkk7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dswh/lil_nlp_with_tensorflow/blob/main/04_02_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTguFckTEDWd"
      },
      "source": [
        "# Introduction to Text generation\n",
        "\n",
        "This notebook explains how we can split a given corpus of data into features and labels and then train a neural network to predict the next word in a sentence.\n",
        "\n",
        "1. Create a corpus - break the text down to list of sentences.\n",
        "2. Create a word_index(vocabulary) from the text.\n",
        "3. Tokenize the data and create n-gram sequence for each sequence of the corpus.\n",
        "4. Pad those sequences.\n",
        "5. Segregate features from the sequences by reserving the last element of the array as labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mW3Mt2q5kL2"
      },
      "source": [
        "##import the required libraries and APIs\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFhZpNjHoxSt"
      },
      "source": [
        "## Step 1: Create a corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwT0yxfRgZY_"
      },
      "source": [
        "data = \"October arrived, spreading a damp chill over the grounds and into the castle.\\n Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students.\\n Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy.\\n The steam pouring from under her vivid hair gave the impression that her whole head was on fire.\\n Raindrops the size of bullets thundered on the castle windows for days on end; the lake rose, the flower beds turned into muddy streams, and Hagrid's pumpkins swelled to the size of garden sheds.\\n Oliver Wood's enthusiasm for regular training sessions, however, was not dampened, which was why Harry was to be found, late one stormy Saturday afternoon a few days before Halloween, returning to Gryffindor Tower, drenched to the skin and splattered with mud.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYtWr3OAoz53"
      },
      "source": [
        "##instantiate tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "##create corpus by lowering the letters and splitting the text by \\n\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce_7Op3eHPS"
      },
      "source": [
        "## Step 2: Train the tokenizer and create word encoding dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAoHN0Ar01tt"
      },
      "source": [
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "##calculate vocabulary size - +1 for <oov> token\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t0633pzeA5y"
      },
      "source": [
        "## Step 3: Create N-gram sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uEYLlk8ra-O"
      },
      "source": [
        "##create n-gram sequences of each text sequence\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    tokens = tokenizer.texts_to_sequences([line])[0]  # get all the tokens of the sequence\n",
        "    for i in range(1, len(tokens)):  # create n-gram sequences\n",
        "        n_gram_sequence = tokens[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zzrLngux8Bd"
      },
      "source": [
        "##pad sequences\n",
        "max_seq_len = max([len(i) for i in input_sequences])\n",
        "input_seq_array = np.array(pad_sequences(input_sequences,\n",
        "                                         maxlen=max_seq_len,\n",
        "                                         padding='pre')\n",
        "                        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJiCqSqYeQDM"
      },
      "source": [
        "## Step 4: Extract features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGbnKtG1zqD"
      },
      "source": [
        "##creating features(X) and label(y)\n",
        "X = input_seq_array[:, :-1]\n",
        "labels = input_seq_array[:, -1]\n",
        "\n",
        "##one-hot encode the labels to get y\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_GvVM22Wsm"
      },
      "source": [
        "print(tokenizer.word_index['mud'])\n",
        "print(X[0])\n",
        "print(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFLkSjPoeg-B"
      },
      "source": [
        "## Define the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95bU1pLN5c0s"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Embedding(vocab_size, 64, input_length=max_seq_len-1),\n",
        "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "                tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(X, y, epochs=500, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDpFBjlzm1Up"
      },
      "source": [
        "## Visualize metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc7nqPbg5tBy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_metric(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHHODnWIrobv"
      },
      "source": [
        "plot_metric(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GLsNdvSm3uv"
      },
      "source": [
        "## Generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db3z5YdkrtXI"
      },
      "source": [
        "seed_text = \"It was a cold night.\"\n",
        "\n",
        "##add number of words you want to predict\n",
        "next_words = 100\n",
        "  \n",
        "##run the loop to predict and concatenate the word\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
        " \n",
        "    ##predict the class using the trained model\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "        ##reference the predicted class with the vocabulary\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-L6m0a6u14X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}