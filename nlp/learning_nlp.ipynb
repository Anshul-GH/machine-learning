{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike, integer or None, not module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-de9e728c6ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not module"
     ]
    }
   ],
   "source": [
    "os.listdir(nltk.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.[a] Some popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however, this definition is rejected by major AI researchers.[b]\n",
    "AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).[2][citation needed] As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.[3] For instance, optical character recognition is frequently excluded from things considered to be AI,[4] having become a routine technology.[5]\n",
    "Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism,[6][7] followed by disappointment and the loss of funding (known as an \"AI winter\"),[8][9] followed by new approaches, success and renewed funding.[7][10] AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI_tokens = word_tokenize(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 26, 'the': 13, 'and': 13, '[': 13, ']': 13, 'ai': 9, 'as': 9, '.': 9, '(': 7, ')': 7, ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    fdist[word.lower()] += 1\n",
    "\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 26),\n",
       " ('the', 13),\n",
       " ('and', 13),\n",
       " ('[', 13),\n",
       " (']', 13),\n",
       " ('ai', 9),\n",
       " ('as', 9),\n",
       " ('.', 9),\n",
       " ('(', 7),\n",
       " (')', 7)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial: artifici\n",
      "intelligence: intellig\n",
      "(: (\n",
      "AI: ai\n",
      "): )\n",
      "is: is\n",
      "intelligence: intellig\n",
      "demonstrated: demonstr\n",
      "by: by\n",
      "machines: machin\n",
      ",: ,\n",
      "as: as\n",
      "opposed: oppos\n",
      "to: to\n",
      "natural: natur\n",
      "intelligence: intellig\n",
      "displayed: display\n",
      "by: by\n",
      "animals: anim\n",
      "including: includ\n",
      "humans: human\n",
      ".: .\n",
      "Leading: lead\n",
      "AI: ai\n",
      "textbooks: textbook\n",
      "define: defin\n",
      "the: the\n",
      "field: field\n",
      "as: as\n",
      "the: the\n",
      "study: studi\n",
      "of: of\n",
      "``: ``\n",
      "intelligent: intellig\n",
      "agents: agent\n",
      "'': ''\n",
      ":: :\n",
      "any: ani\n",
      "system: system\n",
      "that: that\n",
      "perceives: perceiv\n",
      "its: it\n",
      "environment: environ\n",
      "and: and\n",
      "takes: take\n",
      "actions: action\n",
      "that: that\n",
      "maximize: maxim\n",
      "its: it\n",
      "chance: chanc\n",
      "of: of\n",
      "achieving: achiev\n",
      "its: it\n",
      "goals: goal\n",
      ".: .\n",
      "[: [\n",
      "a: a\n",
      "]: ]\n",
      "Some: some\n",
      "popular: popular\n",
      "accounts: account\n",
      "use: use\n",
      "the: the\n",
      "term: term\n",
      "``: ``\n",
      "artificial: artifici\n",
      "intelligence: intellig\n",
      "'': ''\n",
      "to: to\n",
      "describe: describ\n",
      "machines: machin\n",
      "that: that\n",
      "mimic: mimic\n",
      "``: ``\n",
      "cognitive: cognit\n",
      "'': ''\n",
      "functions: function\n",
      "that: that\n",
      "humans: human\n",
      "associate: associ\n",
      "with: with\n",
      "the: the\n",
      "human: human\n",
      "mind: mind\n",
      ",: ,\n",
      "such: such\n",
      "as: as\n",
      "``: ``\n",
      "learning: learn\n",
      "'': ''\n",
      "and: and\n",
      "``: ``\n",
      "problem: problem\n",
      "solving: solv\n",
      "'': ''\n",
      ",: ,\n",
      "however: howev\n",
      ",: ,\n",
      "this: thi\n",
      "definition: definit\n",
      "is: is\n",
      "rejected: reject\n",
      "by: by\n",
      "major: major\n",
      "AI: ai\n",
      "researchers: research\n",
      ".: .\n",
      "[: [\n",
      "b: b\n",
      "]: ]\n",
      "AI: ai\n",
      "applications: applic\n",
      "include: includ\n",
      "advanced: advanc\n",
      "web: web\n",
      "search: search\n",
      "engines: engin\n",
      "(: (\n",
      "e.g.: e.g.\n",
      ",: ,\n",
      "Google: googl\n",
      "): )\n",
      ",: ,\n",
      "recommendation: recommend\n",
      "systems: system\n",
      "(: (\n",
      "used: use\n",
      "by: by\n",
      "YouTube: youtub\n",
      ",: ,\n",
      "Amazon: amazon\n",
      "and: and\n",
      "Netflix: netflix\n",
      "): )\n",
      ",: ,\n",
      "understanding: understand\n",
      "human: human\n",
      "speech: speech\n",
      "(: (\n",
      "such: such\n",
      "as: as\n",
      "Siri: siri\n",
      "and: and\n",
      "Alexa: alexa\n",
      "): )\n",
      ",: ,\n",
      "self-driving: self-driv\n",
      "cars: car\n",
      "(: (\n",
      "e.g.: e.g.\n",
      ",: ,\n",
      "Tesla: tesla\n",
      "): )\n",
      ",: ,\n",
      "automated: autom\n",
      "decision-making: decision-mak\n",
      "and: and\n",
      "competing: compet\n",
      "at: at\n",
      "the: the\n",
      "highest: highest\n",
      "level: level\n",
      "in: in\n",
      "strategic: strateg\n",
      "game: game\n",
      "systems: system\n",
      "(: (\n",
      "such: such\n",
      "as: as\n",
      "chess: chess\n",
      "and: and\n",
      "Go: go\n",
      "): )\n",
      ".: .\n",
      "[: [\n",
      "2: 2\n",
      "]: ]\n",
      "[: [\n",
      "citation: citat\n",
      "needed: need\n",
      "]: ]\n",
      "As: as\n",
      "machines: machin\n",
      "become: becom\n",
      "increasingly: increasingli\n",
      "capable: capabl\n",
      ",: ,\n",
      "tasks: task\n",
      "considered: consid\n",
      "to: to\n",
      "require: requir\n",
      "``: ``\n",
      "intelligence: intellig\n",
      "'': ''\n",
      "are: are\n",
      "often: often\n",
      "removed: remov\n",
      "from: from\n",
      "the: the\n",
      "definition: definit\n",
      "of: of\n",
      "AI: ai\n",
      ",: ,\n",
      "a: a\n",
      "phenomenon: phenomenon\n",
      "known: known\n",
      "as: as\n",
      "the: the\n",
      "AI: ai\n",
      "effect: effect\n",
      ".: .\n",
      "[: [\n",
      "3: 3\n",
      "]: ]\n",
      "For: for\n",
      "instance: instanc\n",
      ",: ,\n",
      "optical: optic\n",
      "character: charact\n",
      "recognition: recognit\n",
      "is: is\n",
      "frequently: frequent\n",
      "excluded: exclud\n",
      "from: from\n",
      "things: thing\n",
      "considered: consid\n",
      "to: to\n",
      "be: be\n",
      "AI: ai\n",
      ",: ,\n",
      "[: [\n",
      "4: 4\n",
      "]: ]\n",
      "having: have\n",
      "become: becom\n",
      "a: a\n",
      "routine: routin\n",
      "technology: technolog\n",
      ".: .\n",
      "[: [\n",
      "5: 5\n",
      "]: ]\n",
      "Artificial: artifici\n",
      "intelligence: intellig\n",
      "was: wa\n",
      "founded: found\n",
      "as: as\n",
      "an: an\n",
      "academic: academ\n",
      "discipline: disciplin\n",
      "in: in\n",
      "1956: 1956\n",
      ",: ,\n",
      "and: and\n",
      "in: in\n",
      "the: the\n",
      "years: year\n",
      "since: sinc\n",
      "has: ha\n",
      "experienced: experienc\n",
      "several: sever\n",
      "waves: wave\n",
      "of: of\n",
      "optimism: optim\n",
      ",: ,\n",
      "[: [\n",
      "6: 6\n",
      "]: ]\n",
      "[: [\n",
      "7: 7\n",
      "]: ]\n",
      "followed: follow\n",
      "by: by\n",
      "disappointment: disappoint\n",
      "and: and\n",
      "the: the\n",
      "loss: loss\n",
      "of: of\n",
      "funding: fund\n",
      "(: (\n",
      "known: known\n",
      "as: as\n",
      "an: an\n",
      "``: ``\n",
      "AI: ai\n",
      "winter: winter\n",
      "'': ''\n",
      "): )\n",
      ",: ,\n",
      "[: [\n",
      "8: 8\n",
      "]: ]\n",
      "[: [\n",
      "9: 9\n",
      "]: ]\n",
      "followed: follow\n",
      "by: by\n",
      "new: new\n",
      "approaches: approach\n",
      ",: ,\n",
      "success: success\n",
      "and: and\n",
      "renewed: renew\n",
      "funding: fund\n",
      ".: .\n",
      "[: [\n",
      "7: 7\n",
      "]: ]\n",
      "[: [\n",
      "10: 10\n",
      "]: ]\n",
      "AI: ai\n",
      "research: research\n",
      "has: ha\n",
      "tried: tri\n",
      "and: and\n",
      "discarded: discard\n",
      "many: mani\n",
      "different: differ\n",
      "approaches: approach\n",
      "since: sinc\n",
      "its: it\n",
      "founding: found\n",
      ",: ,\n",
      "including: includ\n",
      "simulating: simul\n",
      "the: the\n",
      "brain: brain\n",
      ",: ,\n",
      "modeling: model\n",
      "human: human\n",
      "problem: problem\n",
      "solving: solv\n",
      ",: ,\n",
      "formal: formal\n",
      "logic: logic\n",
      ",: ,\n",
      "large: larg\n",
      "databases: databas\n",
      "of: of\n",
      "knowledge: knowledg\n",
      "and: and\n",
      "imitating: imit\n",
      "animal: anim\n",
      "behavior: behavior\n",
      ".: .\n",
      "In: in\n",
      "the: the\n",
      "first: first\n",
      "decades: decad\n",
      "of: of\n",
      "the: the\n",
      "21st: 21st\n",
      "century: centuri\n",
      ",: ,\n",
      "highly: highli\n",
      "mathematical: mathemat\n",
      "statistical: statist\n",
      "machine: machin\n",
      "learning: learn\n",
      "has: ha\n",
      "dominated: domin\n",
      "the: the\n",
      "field: field\n",
      ",: ,\n",
      "and: and\n",
      "this: thi\n",
      "technique: techniqu\n",
      "has: ha\n",
      "proved: prove\n",
      "highly: highli\n",
      "successful: success\n",
      ",: ,\n",
      "helping: help\n",
      "to: to\n",
      "solve: solv\n",
      "many: mani\n",
      "challenging: challeng\n",
      "problems: problem\n",
      "throughout: throughout\n",
      "industry: industri\n",
      "and: and\n",
      "academia: academia\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "for word in AI_tokens:\n",
    "    print(word + \": \" + pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giver: giver\n",
      "given: given\n",
      "gave: gave\n",
      "giving: give\n"
     ]
    }
   ],
   "source": [
    "words_to_stem = ['give', 'giver', 'given', 'gave', 'giving']\n",
    "for word in words_to_stem:\n",
    "    print(word + \": \" + pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: giv\n",
      "giver: giv\n",
      "given: giv\n",
      "gave: gav\n",
      "giving: giv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print(word + \": \" + lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giver: giver\n",
      "given: given\n",
      "gave: gave\n",
      "giving: give\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sst = SnowballStemmer('english')\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print(word + \": \" + sst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import wordnet, WordNetLemmatizer\n",
    "\n",
    "word_lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give: give\n",
      "giver: giver\n",
      "given: given\n",
      "gave: gave\n",
      "giving: giving\n"
     ]
    }
   ],
   "source": [
    "for word in words_to_stem:\n",
    "    print(word + \": \" + word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anshul/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc384f7e1c802d762e8ae2d35493986cf4d8dfda21fbfcf3845d3480326ba56e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
