{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optimum-bishop",
   "metadata": {},
   "source": [
    "# <center>spaCy's Pipelines</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-species",
   "metadata": {},
   "source": [
    "<center>Dr. W.J.B. Mattingly</center>\n",
    "\n",
    "<center>Smithsonian Data Science Lab and United States Holocaust Memorial Museum</center>\n",
    "\n",
    "<center>August 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-humanitarian",
   "metadata": {},
   "source": [
    "In this notebook, we will be learning about the various pipelines in spaCy. As we have seen, spaCy offers both heuristic (rules-based) and machine learning natural language processing solutions. These solutions are activated by pipes. In this notebook, you will learn about pipes and pipelines generally and the ones offered by spaCy specifically. In a later notebook, we will explore how you can create custom pipes and add them to a spaCy pipeline. Before we jump in, let's import spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alike-numbers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-haiti",
   "metadata": {},
   "source": [
    "## Standard Pipes (Components and Factories) Available from spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-guess",
   "metadata": {},
   "source": [
    "SpaCy is much more than an NLP framework. It is also a way of designing and implementing complex pipelines. A **pipeline** is a sequence of **pipes**, or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier pipes. In other cases, a pipe can exist entirely on its own. An example can be see in the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-democrat",
   "metadata": {},
   "source": [
    "```{image} ./images/sample_pipeline.png\n",
    ":alt: pipeline\n",
    ":class: bg-primary\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-insured",
   "metadata": {},
   "source": [
    "Here, we see an input, in this case a sentence, enter the pipeline from the left. Two pipes are activated on this, a rules-based named entity recognizer known as an EntityRuler which finds entities and an EntityLinker pipe that identifies what entity that is to perform toponym resolution. The sentence is then outputted with the sentence and the entities annotated. At this point, we could use the doc.ents feature to find the entities in our sentence. In spaCy, you will often use pipelines that are more sophisticated than this. You will specifically use a Tok2Vec input layer to vectorize your input sentence. This will allow machine learning pipes to make predictions.\n",
    "\n",
    "Below is a complete list of the AttributeRuler pipes available to you from spaCy and the Matchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-segment",
   "metadata": {},
   "source": [
    "### Attribute Rulers\n",
    "* Dependency Parser\n",
    "* EntityLinker\n",
    "* EntityRecognizer\n",
    "* EntityRuler\n",
    "* Lemmatizer\n",
    "* Morpholog\n",
    "* SentenceRecognizer\n",
    "* Sentencizer\n",
    "* SpanCategorizer\n",
    "* Tagger\n",
    "* TextCategorizer\n",
    "* Tok2Vec\n",
    "* Tokenizer\n",
    "* TrainablePipe\n",
    "* Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-positive",
   "metadata": {},
   "source": [
    "### Matchers\n",
    "* DependencyMatcher\n",
    "* Matcher\n",
    "* PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-seating",
   "metadata": {},
   "source": [
    "## How to Add Pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-liberal",
   "metadata": {},
   "source": [
    "In most cases, you will use an off-the-shelf spaCy model. In some cases, however, an off-the-shelf model will not fill your needs or will perform a specific task very slowly. A good example of this is sentence tokenization. Imagine if you had a document that was around 1 million sentences long. Even if you used the small English model, your model would take a long time to process those 1 million sentences and separate them. In this instance, you would want to make a blank English model and simply add the Sentencizer to it. The reason is because each pipe in a pipeline will be activated (unless specified) and that means that each pipe from Dependency Parser to named entity recognition will be performed on your data. This is a serious waste of computational resources and time. The small model may take hours to achieve this task. By creating a blank model and simply adding a Sentencizer to it, you can reduce this time to merely minutes.\n",
    "\n",
    "To demonstrate this process, let's first create a blank model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consistent-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-toolbox",
   "metadata": {},
   "source": [
    "Here, notice that we have used spacy.blank, rather than spacy.load. When we create a blank model, we simply pass the two letter combination for a language, in this case, en for English. Now, let's use the add_pipe() command to add a new pipe to it. We will simply add a sentencizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confident-arena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x2167b5468c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deadly-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "olive-reynolds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94133\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approximate-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp2 = spacy.load(\"en_core_web_sm\")\n",
    "nlp2.max_length = 5278439"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "healthy-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112074\n",
      "Wall time: 47min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp2(soup)\n",
    "print (len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-pencil",
   "metadata": {},
   "source": [
    "The difference in time here is remarkable. Our text string was around 5.2 million characters. The blank model with just the Sentencizer completed its task in 7.54 seconds and found around 94k sentences. The small English model, the most efficient one offered by spaCy, did the same task in 46 minutes and 15 seconds and found around 112k sentences. The small English model, in other words, took approximately 380 times longer. \n",
    "\n",
    "Often times you need to find sentences quickly, not necessarily accurately. In these instances, it makes sense to know tricks like the one above. This notebook concludes part one of this book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-genius",
   "metadata": {},
   "source": [
    "## Examining a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-squad",
   "metadata": {},
   "source": [
    "In spaCy, we have a few different ways to study a pipeline. If we want to do this in a script, we can do the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "missing-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2.analyze_pipes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-residence",
   "metadata": {},
   "source": [
    "Note the dictionary structure. This tells us not only what is inside the pipeline, but its order. Each key after \"summary\" is a pipe. The value is a dictionary. This dictionary tells us a few different things. All of these value dictionaries state: \"assigns\" which corresponds to a value of what that particular pipe assigns to the token and doc as it passes through the pipeline. In some cases, there will be a key of \"scores\" in the dictionary. This indicates how the machine learning model was evaluated. We will learn more about model evaluation in our machine learning section below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-midwest",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-cosmetic",
   "metadata": {},
   "source": [
    "This notebook concludes part one of this book. It has given you an umbrella overview of spaCy. Over the next few parts of this book, we will deep dive into specific areas and use spaCy to solve general and domain-specific problems from several different areas of industry. Join me as we learn to create custom models and do custom things to leverage the full potential of the spaCy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-cooking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
